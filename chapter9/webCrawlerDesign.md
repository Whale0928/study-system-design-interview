# 9장 - 웹크롤러설계

### **1 .  웹 크롤러**

> 웹 크롤러는 로봇 또는 스파이더라고도 부릅니다.
특정 몇 개의 웹 페이지에서 시작하여 페이지에 포함된 링크를 따라 
다른 페이지로 이동하면서 연속적으로 *콘텐츠를 수집합니다.

*콘텐츠 : 웹 페이지, 이미지, 비디오, PDF 파일 등
> 

1. **검색엔진 인덱싱**:
    - **목적**: 사용자가 특정 키워드로 검색할 때 관련된 웹 페이지를 빠르게 찾기 위해 웹 페이지 내용을 분석하고 저장합니다.
    - **방법**: 웹 크롤러가 인터넷을 탐색하며 페이지의 텍스트와 링크를 분석해 인덱스를 생성합니다.
    - **이용 매체/서비스**: Google, Bing, Yahoo! 같은 검색엔진.
2. **웹 아카이빙**:
    - **목적**: 웹 페이지의 내용을 정기적으로 보관하여, 페이지가 변경되거나 사라졌을 때 이전 버전을 조회할 수 있도록 합니다.
    - **방법**: 웹 아카이브 서비스가 정기적으로 웹 페이지를 크롤링해 스냅샷을 저장합니다.
    - **이용 매체/서비스**: 인터넷 아카이브(Wayback Machine), 국립도서관 웹 아카이빙 서비스.
3. **웹 마이닝**:
    - **목적**: 웹 데이터에서 유용한 정보나 패턴을 발견하기 위해 데이터 마이닝 기술을 적용합니다. 마케팅, 사용자 행동 분석 등에 활용됩니다.
    - **방법**: 수집된 데이터에 통계적, 기계 학습적 방법을 적용해 패턴을 찾아내고 분석합니다.
    - **이용 매체/서비스**: 온라인 마케팅 회사, 소셜 미디어 플랫폼, e-커머스 사이트.
4. **웹 모니터링**:
    - **목적**: 특정 웹사이트나 웹 페이지의 변경사항을 지속적으로 감시하고, 중요한 업데이트나 조건 충족 시 알림을 받습니다.
    - **방법**: 지정된 페이지를 정기적으로 검사하며, 내용 변경이나 특정 키워드의 출현을 확인합니다.
    - **이용 매체/서비스**: 뉴스 기관, 경쟁사 분석 도구, 웹사이트 성능 모니터링 서비스.

### **1 - 2. 웹 크롤러를 이용한** 웹 크롤링과 웹 스크래핑

> 크롤러는 웹 사이트를 탐색하고 데이터를 추출하는 작업을 수행할 때 
이 두 가지 작업을 통합적으로 사용할 수 있는 도구입니다.
> 

- **웹 크롤링 (Web Crawling)**:
    - 크롤러는 웹 사이트를 자동으로 탐색하면서 페이지의 URL을 수집합니다.
    - 수집된 URL은 크롤러가 다음에 방문할 페이지를 결정하는 데 사용됩니다.
    - 웹 크롤링은 주로 웹 사이트의 구조를 파악하고 인덱싱하기 위한 목적으로 사용됩니다.
- **웹 스크래핑 (Web Scraping)**:
    - 크롤러가 웹 페이지에 접근하면 해당 페이지의 HTML 또는 XML 코드를 다운로드합니다.
    - 이 코드를 분석하여 필요한 데이터를 추출하고 저장합니다.
    - 웹 스크래핑은 데이터 수집, 분석, 보고서 작성 등의 목적으로 사용됩니다.

### **1 - 3.**  웹 크롤러의 기본 알고리즘과 대규모 웹 환경에서의 크롤러

> 웹 크롤러의 설계는 표면적으로 단순해 보이는 작업이지만, 
실제로는 다양한 복잡한 요소들을 고려해야 하는 매우 어려운 작업입니다.
> 

**기본알고리즘 :**

1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.

2. 다운받은 웹 페이지에서 URL들을 추출한다.

3.추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

**대규모 웹 환경에서의 크롤러 고려사항: (뒤에서 다룸)**
4가지 주요 고려사항들은 웹 크롤러가 효과적이고 인터넷의 방대한 정보를 
수집하고 처리할 수 있도록 하기 위해 생겨났습니다. 

- **규모 확장성**
    - **이유**: 웹의 규모가 수시로 증가하므로, 크롤러는 대규모 데이터를 처리할 수 있는 능력이 필수적임.
    - **목적**: 크롤러가 시간이 지남에 따라 증가하는 데이터 양을 지속적으로 처리할 수 있도록, 자동화된 확장 메커니즘과 효율적인 자원 관리 기능을 갖추는 것.
    - **구체적 방안**: 클라우드 기반 인프라 사용, 자동화된 확장성 관리 시스템 구현, 분산 처리 기술 적용.
- **안정성**
    - **이유**: 웹은 다양한 형식과 예측 불가능한 문제를 포함하므로, 크롤러는 이를 견딜 수 있어야 함.
    - **목적**: 예기치 않은 오류 발생 시에도 크롤러가 계속 작동할 수 있도록, 견고한 오류 처리 및 복구 메커니즘을 갖추는 것.
    - **구체적 방안**: 오류 로깅, 자동 재시도 메커니즘, 타임아웃 및 예외 처리 전략 구현.
- **예절**
    - **이유**: 과도한 요청은 웹 서버에 부담을 줄 수 있으므로, 크롤러는 서버에 대한 존중을 유지해야 함.
    - **목적**: 웹사이트의 정상적인 운영을 방해하지 않으면서 효과적으로 데이터를 수집하기 위해, 적절한 크롤링 속도 및 접근 방법을 유지하는 것.
    - **구체적 방안**: Crawl-delay 규칙 준수, 동적 요청 속도 조정, robots.txt 준수.
- **확장성**
    - **이유**: 웹은 지속적으로 새로운 형식과 기술로 발전하고 있으므로, 크롤러는 이에 적응할 수 있어야 함.
    - **목적**: 새롭고 다양한 형태의 웹 콘텐츠를 신속하게 인식하고 처리할 수 있도록, 유연한 아키텍처 및 적응형 파싱 기능을 갖추는 것.
    - **구체적 방안**: 모듈화된 구조 설계, 플러그인이나 확장 가능한 컴포넌트를 사용한 기능 추가, 다양한 데이터 형식 처리 능력 강화.

### **1 - 4. 웹 크롤러 윤리와 합법적으로 웹 크롤링을 하는 방법**

> 너무 길어져서 링크 참조
> 1편 :  [https://yozm.wishket.com/magazine/detail/878/](https://yozm.wishket.com/magazine/detail/878/)
> 2편 : [https://yozm.wishket.com/magazine/detail/877/]/](https://yozm.wishket.com/magazine/detail/877/%5D/)
> 

### 2 **.  계략적 규모 추정과 설계**

> - 매달 10억 개의 웹 페이지를 다운로드 한다.
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400 page / sec
- 최대 (Peak) QPS = 2 X QPS = 800 page/sec
- 웹 페이지의 평균 크기는 500k라고 가정
- 10억 페이지 x 500k = 500 TB /month.
- 1개월치 데이터를 보관하는 데는 500 TB, 5년간 보관한다고 
   가정하면 결국 500TB X 12개월 X 5년 = 30PB의 저장용량이 필요하다.
> 

위의 요구사항대로 계략적 설계를 하면 아래 이미지처럼 된다.

![Untitled](https://github.com/organization-for-study/study-system-design-interview/assets/97773895/6272bf09-66cb-4fa7-bf59-85d502d42033)


- **시작 URL 집합**: 
크롤러가 크롤링을 시작하는 지점입니다. 
전체 웹을 크롤링하는 경우, 다양한 출발점을 선정합니다.
크롤러가 가능한 많은 링크를 탐색하도록 합니다. 
주제별로 시작 URL을 다르게 설정하여 URL 공간을 세분화할 수도 있습니다.
- **미수집 URL 저장소**: 
아직 방문하지 않은 URL을 관리하는 저장소입니다.
FIFO(First-In-First-Out) 큐로 구현됩니다.
- **HTML 다운로더**: 
웹 페이지를 다운로드하는 컴포넌트입니다.
미수집 URL 저장소로부터 URL을 받아 작업합니다.
- **도메인 이름 변환기**: 
웹 페이지를 다운로드하기 전에 URL을 해당  IP 주소로 변환해야 합니다.
- **콘텐츠 파서**: 
다운로드된 웹 페이지를 파싱하고 검증합니다.
이상한 웹 페이지로 인한 문제를 방지하고 저장 공간을 효율적으로 사용합니다. 
독립된 컴포넌트로 구성됩니다.
- **콘텐츠 저장소**: 
HTML 문서를 저장하는 시스템으로, 다양한 요소를 고려하여 구현해야 합니다. 
대부분의 콘텐츠는 디스크에 저장하고, 인기 있는 콘텐츠는 메모리에 저장합니다.
접근 지연시간을 줄입니다.
- **URL 추출기**: 
HTML 페이지에서 링크를 파싱하여 추출합니다.
- **URL 필터**: 
크롤링 대상에서 특정한 조건을 만족하는 URL을 제외합니다. 
예를 들어, 
특정 콘텐츠 타입이나 파일 확장자를 가진 URL, 
오류가 발생하는 URL, 접근이 제외된 URL 등을 필터링합니다.
- **이미 방문한 URL?**
이미 방문한URL은 방문하지않고, 미수집 URL저장소로 가서
새로운 URL을 크롤링합니다.
( 블룸 필터, 해시 테이블 사용됨 )
- **URL 저장소**: 
새로운 링크는 저장합니다.

*URL저장소에 접근하기전에, 미수집 URL저장소에서 방문한 URL인지 체크하는게 효율적이지않은가?

### 3 - 1 **.  상세 설계**

> - DFS(Depth-First Search)와 BFS(Breadth-First Search)
- 미수집 URL저장소
- HTML다운로더
- 안정성 확보 전략
- 확장성 확보 전략
- 문제있는 콘텐츠 감지 및 회피전략
> 

### 3 - 1 **.**  DFS와 BFS

> DFS(Depth-First Search)와 BFS(Breadth-First Search)는
시작 URL 집합으로부터 크롤링을 시작한 후 어떻게 웹을 탐색할지 결정하는 데 사용됩니다.
> 

<img width="713" alt="스크린샷 2024-01-03 오후 8 59 44" src="https://github.com/organization-for-study/study-system-design-interview/assets/97773895/1318813e-b52b-4816-b333-013f146b0028">


- **BFS 방식을 사용할 경우 :**
웹의 넓은 영역을 폭넓게 탐색하려면 BFS가 유리합니다. 
즉, BFS는 같은 레벨의 다른 URL들을 방문합니다.(병렬적 처리)
규모가 큰 웹사이트에서는 BFS를 사용하여 레벨별로 정보를 수집하는 것이 더 효율적일 수 있습니다.
FIFO를 사용하는데 한쪽은 탐색할 URL을 놓고, 한쪽은 꺼내기만 합니다.
    - **문제점:
    병렬적으로 처리하기때문에** 특정 시간에 많은 수의 페이지를 빠르게 요청하게 만듭니다.
    대규모 사이트나 인기 있는 서비스에서 크롤링을 할 때 해당 사이트의 서버에 
    상당한 부하를 줄 수 있습니다. ( 예의없음 )
        - **속도 제한**: 
        크롤러가 요청을 보내는 속도를 제한합니다. 한 시간에 서버에 보내는 요청의 수를 조절합니다.
        - **폴리트니스(politeness) 정책**: 
        robots.txt 파일과 HTTP 헤더를 확인하여 사이트의 크롤링 지침을 따릅니다.
        "Crawl-delay" 지침과 같은 것을 존중합니다.
        - **동적 크롤링**: 
        사이트의 응답 시간을 모니터링하고, 서버가 느려질 때 자동으로 요청 속도를 줄입니다.
- **DFS 방식을 사용할 경우 :** 
목적이 웹의 특정 부분에 대한 심층 정보를 수집하는 것이라면 DFS가 적합할 수 있습니다. 
****크롤러는 시작 URL 집합 중 하나에서 시작하여 가능한 한 깊게 링크를 따라 내려갑니다. 
DFS는 하나의 경로를 따라 링크의 최대 깊이까지 방문한 후에야 다른 경로를 탐색합니다.
    - **문제점**: 
    순환 경로(cycles) 또는 매우 깊은 경로에서 비효율적입니다.
    그리고 전체적인 구조를 파악하기 어렵습니다.
    (웹 사이트가 다양한 주제와 섹션으로 구성되어 있는지 모름)

### 3 - 2 **.  미수집URL저장소 [ 예의 ]**

> 우선순위와 신선도로 필터링하는
미수집 URL저장소를 잘 구현하면 예의있는 크롤러가된다.
- 예의
- 우선순위
- 신선도
- 미수집 URL저장소를 위한 지속성 저장장치
> 

- **왜 예의를 자꾸 언급하는가? :**
    - **서버에 과한 요청을하여** Dos 공격으로 간주될 수 있다.
    - 해당 사이트가 마비될 수 있다.
    - 그래서 한번에 한 페이지만요청해야한다.

- **그러면 너무 느려지지않나..? :**
    - 동시에 여러 개의 크롤러 또는 크롤링 스레드를 실행합니다.
    - 각각 다른 페이지를 요청하면, 전체적인 크롤링 속도를 높일 수 있습니다.
    - 각 스레드는 독립된 FIFO 큐를 가지고 있어 서로 다른 URL을 처리합니다.

![캡처](https://github.com/organization-for-study/study-system-design-interview/assets/97773895/6a39358c-17df-40d5-82c6-2a606e0119de)


즉, 각 다운로드 스레드는 별도의 FIFO 큐를 가지고 있어서 
해당 큐에서 꺼낸 URL만 다운로드한다.
그럼 한번에 한 페이지만 요청해서 큐와 스레드를 이용해서
요청은 한번이지만 해당 페이지에 많은 정보를 예의있게 크롤링 할 수 있다.

- **큐 라우터 :** 
같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할을 한다.
- **매핑 테이블 :** 
호스트 이름과 큐 사이의 관계를 보관하는 테이블이다.
- **FIFO 큐 :** 
같은 호스트에 속한 URl은 언제나 같은 큐에 보관된다.
- **큐 선택기 :** 
큐 선택기는 큐들을 순회하면서 큐에서 URl을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달하는 역할을 한다.
- **작업 스레드 :** 
작업 스레드는 전달된 URL을 다운로드하는 작업을 수행한다. 전달된 URL은 순차적으로 처리될 것이며, 작업들 사이에는 일정한 지연시간을 둘 수 있다.

### 3 - 2 **.  미수집URL저장소 [** 우선순위 **]**

> 우선순위와 신선도로 필터링하는
미수집 URL저장소를 잘 구현하면 예의있는 크롤러가된다.
- 예의
- 우선순위
- 신선도
- 미수집 URL저장소를 위한 지속성 저장장치
>

<img width="412" alt="Untitled (1)" src="https://github.com/organization-for-study/study-system-design-interview/assets/97773895/beb3ea47-9ac0-404d-a54f-dae5cbb75014">

- 입력된 URL이 순위결정장치를 통과하여 여러 FIFO 큐 중 하나로 배정됩니다.
- 큐 선택기가 큐에서 URL을 선택하여 크롤링을 진행합니다.
- 순위결정장치는 일반적으로 다음과 같은 기준에 기반하여 우선 순위를 결정합니다:
    - **URL의 중요성**: 
    특정 페이지의 중요성이나 인기도를 기준으로 할 수 있습니다.
    예를 들어, 홈페이지나 사이트맵 같은 고정적으로 중요한 페이지들을 먼저 크롤링할 수 있습니다.
    - **콘텐츠의 신선도**: 
    최근에 변경된 페이지나 자주 업데이트되는 페이지를 우선적으로 크롤링합니다.
    - **사이트의 크롤링 정책**: 
    **robots.txt** 파일이나 메타 태그에서 지정한 크롤링 지침에 따라 우선 순위를 조정합니다.
    - **백링크 수**: 
    다른 웹사이트로부터 링크된 횟수가 많은 페이지는 더 높은 우선 순위를 가질 수 있습니다.
    - **사용자 정의 규칙**: 
    특정 주제, 키워드 또는 다른 사용자 정의 조건에 따라 우선 순위를 결정합니다.

**[ 완전체 ]**

<img width="412" alt="Untitled (1)" src="https://github.com/organization-for-study/study-system-design-interview/assets/97773895/2ec30248-39f7-46bf-aaeb-f1877ae23ce1">


- 전면큐(front queue) : 우선순위 결정 과정을 처리한다.
- 후면큐(back queue) : 크롤러가 예의 바르게 동작하도록 보증한다.

### 3 - 3 **.**  미수집 URL저장소를 위한 지속성 저장장치

> 대부분의 URL은 디스크에 두지만  IO(input/output)비용을 줄이기위해
메모리 버퍼에 큐를 둔다.
버퍼에 있는 데이터는 주기적으로 디스크에 기록해둔다.
> 

### 3 - 4 **.  HTML다운로더 [ robots.txt ]**

> HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.

Robots.txt는 웹사이트의 루트 디렉토리에 위치하는 텍스트 파일로, 
웹 크롤러가 사이트의 어떤 부분을 방문하거나 스캔해서는 안 되는지 
지시하는 규칙을 정의합니다.

*Robots.txt 파일은 
웹 사이트 소유자 또는 관리자가 선택적으로 작성함(필수는 아님)
아래처럼 작성한다.
> 
> 
> User-agent: [크롤러 또는 검색 엔진 이름]
> Disallow: [크롤링이 금지된 디렉토리 또는 파일 경로]
> Allow: [크롤링이 허용된 디렉토리 또는 파일 경로]
> 

- **HTML 다운로더와 robots.txt의 상호작용 :**
크롤러가 URL을 방문하기 전에, HTML 다운로더는 먼저 해당 사이트의 **robots.txt** 파일을 확인합니다. 
이 파일의 지시사항에 따라 특정 페이지를 다운로드할지 말지를 결정합니다. 
만약 **robots.txt**가 특정 경로의 크롤링을 금지하고 있다면, 예의 바른 크롤러는 
그 페이지를 다운로드해서는 안 됩니다.

### 3 - 5 **.  HTML다운로더 [** 성능 최적화 **]**

> HTML 다운로더를 설계할 때는 성능최적화도 아주 중요하다.
> 

- **분산 크롤링:**

    <img width="307" alt="Untitled (3)" src="https://github.com/organization-for-study/study-system-design-interview/assets/97773895/e51d678d-7cb4-4441-8a16-1380b2c4150b">

    - 크롤링 작업을 여러 서버로 분산시켜 성능을 향상시키는 방법입니다.
    - 각 서버는 여러 스레드를 사용하여 다운로드 작업을 처리합니다
    - URL 공간을 작은 단위로 분할하여 각 서버가 일부 다운로드를 담당한다.
- **도메인 이름 변환 결과 캐시:**
    - DNS 요청에 의한 도메인 이름 변환 작업이 크롤러의 병목 현상 중 하나
    - 그래서 DNS 조회 결과를 캐시에 저장하고 주기적으로 갱신하여 크롤러 성능을 향상시킨다.
    - 이로 인해 DNS 요청으로 인한 대기 시간을 줄일 수 있다.
- **지역성:**
    - 크롤링 서버를 지역별로 분산 배치하여 페이지 다운로드 시간을 최소화하는 방법입니다.
    - 크롤링 서버가 대상 서버와 지역적으로 가까우면 다운로드 시간이 줄어든다.
- **짧은 타임아웃:**
    - 웹 서버가 느리게 응답하거나 아예 응답하지 않는 경우에 대비해 최대 대기 시간을 설정합니다.
    - 이 시간 동안 서버가 응답하지 않으면 크롤러는 해당 페이지 다운로드를 중지하고 다음 페이지로 이동한다.

### 3 - 5 **.  HTML다운로더 [ 안정성** **]**

> 최적화된 성능뿐 아니라 안정성도 다운로더 설계 시 중요하게 고려해야 할 부분이다. 

특히, 수집할 데이터가 많으면 장애가 발생할 수 있으며, 이를 예방하기위해
예외처리를 하고, 서버부하를 줄이는 방법을 고려해야합니다.
그리고 크롤링된 데이터를 검증하는 절차도 필수적입니다.
> 

- 안정 해시(consistent hashing)
    - 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술이다.
    - 안정 해시를 사용하면, 서버가 추가되거나 제거될 때 전체 해시 테이블을 재분배할 필요 없이, 
    최소한의 데이터만 재분배하여 부하를 분산할 수 있습니다.
    - 결과적으로 시스템의 확장성과 관리 용이성이 향상됩니다.
- 크롤링 상태 및 수집 데이터
    - 장애가 발생할 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 
    지속적 저장장치에 기록해 두는 것이 바람직합니다.
    - 시스템 장애 발생 시 빠르게 복구할 수 있습니다.
    - 분산 데이터베이스나 파일 시스템을 활용하여 크롤링 상태를 체크포인트로 저장함으로써, 
    장애 발생 시 마지막 체크포인트에서 작업을 재개할 수 있습니다.
- 예외 처리
    - 대규모 크롤링 시스템은 다양한 예외 상황에 직면할 수 있습니다.
    - 예외 발생 시 시스템의 다른 부분에 영향을 미치지 않고, 
    문제를 격리하여 처리하는 로직을 구현해야 합니다.
    - 로깅, 알림, 자동 재시도 등을 포함한 견고한 에러 핸들링 전략이 필요합니다.
- 데이터 검증
    - 크롤링된 데이터의 정확성과 일관성을 보장하기 위해 데이터 검증 단계를 도입하는 것이 중요합니다.
    - 해시 합계, 체크섬, 데이터 스키마 검증 등을 통해 수집된 데이터가 손상되지 않았는지 확인합니다.
    - 데이터 손상이 발견될 경우, 시스템은 해당 데이터를 다시 크롤링하거나 수정할 수 있어야 합니다.
    

### 3 - 5 **.  HTML다운로더 [ 확장성** **]**

> 최적화된 성능뿐 아니라 안정성도 다운로더 설계 시 중요하게 고려해야 할 부분이다. 

특히, 수집할 데이터가 많으면 장애가 발생할 수 있으며, 이를 예방하기위해
예외처리를 하고, 서버부하를 줄이는 방법을 고려해야합니다.
그리고 크롤링된 데이터를 검증하는 절차도 필수적입니다.
> 

<img width="609" alt="Untitled (4)" src="https://github.com/organization-for-study/study-system-design-interview/assets/97773895/225491fd-8f5a-4d2d-89e7-91173b5fa266">


- **PNG 다운로더**
    - PNG 다운로더는 웹 페이지에서 PNG 형식의 이미지 파일을 다운로드하는 특화된 모듈입니다.
    - 웹 페이지가 HTML 콘텐츠 외에도 다양한 멀티미디어 콘텐츠를 포함할 수 있기 때문에, 특정 파일 형식(이 경우 PNG)을 다루는 전용 다운로더가 필요할 수 있습니다.
    - 이 모듈은 웹 페이지의 HTML을 파싱하여 PNG 이미지의 URL을 찾고, 이를 다운로드 큐에 추가하여 실제 이미지 파일을 로컬 저장소나 데이터베이스에 저장합니다.
- **URL 추출기**
    - URL 추출기는 웹 페이지 내에서 새로운 링크를 찾아내는 컴포넌트입니다.
    - 웹 크롤러는 웹의 연결된 구조를 탐색하기 위해 페이지 내에서 다른 페이지로 연결되는 링크들을 추출합니다.
    - 추출된 URL은 크롤링 대상이 될 수 있으며, 이러한 링크들은 크롤링 범위를 확장하고 웹의 구조를 이해하는 데 중요합니다.
    - 이 모듈은 중복된 링크를 걸러내고, 아직 방문하지 않은 새로운 페이지의 URL을 큐에 추가합니다.
- **웹 모니터**
    - 웹 모니터는 흐름도에 명시적으로 언급되지는 않았지만, 일반적으로 웹 크롤링 시스템에서 중요한 역할을 하는 모듈입니다.
    - 웹 모니터는 웹사이트의 변경 사항을 추적하고, 특정 이벤트나 조건에 따라 알림을 제공할 수 있습니다.
    - 예를 들어, 웹 페이지의 내용이나 상태가 변했을 때 이를 감지하고, 필요한 조치를 취하기 위해 시스템 관리자나 다른 시스템 컴포넌트에 알림을 보낼 수 있습니다.
    - 웹 모니터는 크롤링 프로세스의 안정성을 유지하고, 시스템이 웹사이트의 최신 상태를 반영하도록 하는 데 도움을 줍니다.
    

### 3 - 5 **.  HTML다운로더 [ 문제 있는 콘텐츠 감지 및 회피** **]**

> 문제 있는 콘텐츠를 정확하게 감지하고 회피하는건 크롤러 안정성과도 연관있습니다.
> 

- **중복 콘텐츠**
    - 중복 콘텐츠는 웹사이트에서 동일한 내용이 다른 URL로 접근 가능할 때 발생합니다.
    - 예를 들어, 세션 ID, 사용자 추적 파라미터, 혹은 페이지 내 정렬 옵션 등에 의해 URL이 변형되어 같은 콘텐츠에 대해 수많은 URL이 생성될 수 있습니다.
    - 크롤러의 저장소와 처리 능력을 낭비하게 만듭니다.
    - 중복 콘텐츠를 식별하고 처리하기 위해 체크섬이나 해시값을 사용할 수 있습니다.
    - 다운로드된 페이지의 내용을 짧은 문자열로 변환하여 저장하고, 
    새로 크롤링한 내용의 체크섬과 비교함으로써 중복 여부를 판단합니다.
    나중에 데이터를 읽을 때, 저장되었거나 전송받은 데이터에 대해 다시 체크섬을 계산하고, 
    이를 원래의 체크섬과 비교합니다. 
    만약 두 체크섬 값이 일치하면 데이터가 변형되지 않았음을 의미하며, 
    불일치한다면 데이터가 손상되었거나 오류가 발생했음을 나타냅니다.
- **거미덫 (Spider Trap)**
    - 거미덫은 웹 크롤러가 무한히 생성되는 URL 패턴에 갇히는 상황입니다.
    - 예시로, 
    **`http://example.com/page1**, 
    **http://example.com/page1/page2**, 
    **http://example.com/page1/page2/page3**` 
    등과 같이 계속해서 새로운 하위 페이지가 생성되는 경우가 있습니다. 
    이를 방지하기 위해 크롤러는 URL의 패턴을 분석하고, 무한히 중첩되는 
    디렉토리나 파라미터를 감지하여 거미덫으로부터 벗어날 수 있도록 설계되어야 합니다.
- **데이터 노이즈**
    - 크롤링된 데이터 중 가치가 없는 정보를 데이터 노이즈라고 합니다.
    - 웹사이트의 반복적인 헤더, 푸터, 네비게이션 바와 같은 요소 또는 무관한 광고 등이 될 수 있습니다.
    - 데이터 노이즈를 제거하기 위해 크롤러는 데이터를 클리닝하는 과정을 거쳐야합니다.
    - 특정 HTML 태그를 무시하거나, 내용의 특정 패턴을 인식하여 필터링하는 규칙을 적용합니다.
    - 때때로 머신 러닝과 같은 고급 기술을 활용하여 중요한 콘텐츠를 노이즈로부터 구분하기도 합니다.
